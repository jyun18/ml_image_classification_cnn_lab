{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from PIL import Image\n",
    "from keras import preprocessing\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../datasets/cat_dog_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'new_set', 'test_set', 'training_set']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "os.chdir(cwd)\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training file for cats:  4000\n",
      "length of training file for dogs:  4000\n",
      "length of test file for cats:  1000\n",
      "length of test file for dogs:  1000\n"
     ]
    }
   ],
   "source": [
    "# import the training files for cats and dogs\n",
    "train_cats_files = []\n",
    "train_path_cats = data_dir +\"/training_set/cats/\"\n",
    "for path in os.listdir(train_path_cats):\n",
    "    if '.jpg' in path:\n",
    "        train_cats_files.append(os.path.join(train_path_cats, path))\n",
    "        \n",
    "train_dogs_files = []\n",
    "train_path_dogs = data_dir +\"/training_set/dogs/\"\n",
    "for path in os.listdir(train_path_dogs):\n",
    "    if '.jpg' in path:\n",
    "        train_dogs_files.append(os.path.join(train_path_dogs, path))\n",
    "        \n",
    "print(\"length of training file for cats: \", len(train_cats_files)) \n",
    "print('length of training file for dogs: ', len(train_dogs_files))\n",
    "\n",
    "# import the test files for cats and dogs\n",
    "test_cats_files = []\n",
    "test_path_cats = data_dir +\"/test_set/cats/\"\n",
    "for path in os.listdir(test_path_cats):\n",
    "    if '.jpg' in path:\n",
    "        test_cats_files.append(os.path.join(test_path_cats, path))\n",
    "        \n",
    "test_dogs_files = []\n",
    "test_path_dogs = data_dir +\"/test_set/dogs/\"\n",
    "for path in os.listdir(test_path_dogs):\n",
    "    if '.jpg' in path:\n",
    "        test_dogs_files.append(os.path.join(test_path_dogs, path))\n",
    "\n",
    "print(\"length of test file for cats: \", len(test_cats_files)) \n",
    "print(\"length of test file for dogs: \", len(test_dogs_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training dataset:  (8000, 32, 32, 3)\n",
      "Shape of testing dataset:  (2000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "d = 32 # image dimensions: using 32x32 pixels \n",
    "X_train_orig = np.zeros((8000, d, d, 3), dtype='float32') # length of training dataset is 8000, and each image is 32 x 32 x 3 \n",
    "                                                          # ie: width x height x rgb\n",
    "\n",
    "# converting each image in the training dataset into an array\n",
    "for i in range(4000):                                     \n",
    "    path = train_cats_files[i]\n",
    "    img = preprocessing.image.load_img(path, target_size=(d, d))\n",
    "    X_train_orig[i] = preprocessing.image.img_to_array(img)\n",
    "\n",
    "for i in range(4000,8000):    \n",
    "    path = train_dogs_files[i-4000]\n",
    "    img = preprocessing.image.load_img(path, target_size=(d, d))\n",
    "    X_train_orig[i] = preprocessing.image.img_to_array(img)    \n",
    "\n",
    "print(\"Shape of training dataset: \", X_train_orig.shape)\n",
    "\n",
    "X_test_orig = np.zeros((2000, d, d, 3), dtype='float32') # length of testing dataset is 2000, dimensions same as before\n",
    "\n",
    "# converting each image in the testing dataset into an array\n",
    "for i in range(1000):    \n",
    "    path = test_cats_files[i]\n",
    "    img = preprocessing.image.load_img(path, target_size=(d, d))\n",
    "    X_test_orig[i] = preprocessing.image.img_to_array(img)\n",
    "\n",
    "for i in range(1000,2000):    \n",
    "    path = test_dogs_files[i-1000]\n",
    "    img = preprocessing.image.load_img(path, target_size=(d, d))\n",
    "    X_test_orig[i] = preprocessing.image.img_to_array(img)    \n",
    "\n",
    "print(\"Shape of testing dataset: \", X_test_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_orig\n",
    "X_test = X_test_orig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At position 3 should be a cat: 1.0\n",
      "At position 4002 should be a dog: 0.0\n",
      "shape of Y_train:  (8000,)\n",
      "At position 3 should be a cat: 1.0\n",
      "At position 1002 should be a dog: 0.0\n",
      "shape of Y_test:  (2000,)\n"
     ]
    }
   ],
   "source": [
    "Y_train_orig = np.ones((4000,)) # 1 - 4000 are cat pictures so our label is 1\n",
    "Y_train_orig = np.concatenate((Y_train_orig, np.zeros((4000,)))) # 4000 - 8000 are dog pictures so our label is 0\n",
    "Y_train = Y_train_orig.reshape(-1) # reshape it into an array of shape 8000 x 1\n",
    "\n",
    "print(\"At position 3 should be a cat:\", Y_train[3])\n",
    "print(\"At position 4002 should be a dog:\", Y_train[4002])\n",
    "\n",
    "print(\"shape of Y_train: \", Y_train.shape)\n",
    "\n",
    "\n",
    "Y_test = np.ones((1000,)) # 1 - 1000 are cat pictures, so our label is 1\n",
    "Y_test = np.concatenate((Y_test, np.zeros((1000,)))) # 1000 - 2000 are dog pictures so our label is 0\n",
    "Y_test = Y_test.reshape(-1) # reshape it into an array of shape 2000 x 1\n",
    "\n",
    "print(\"At position 3 should be a cat:\", Y_test[3])\n",
    "print(\"At position 1002 should be a dog:\", Y_test[1002])\n",
    "\n",
    "print(\"shape of Y_test: \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Convert each class label to a vector, either [0, 1] or [1, 0]\n",
    "# [0, 1] if cat\n",
    "# [1, 0] if dog\n",
    "Y_train = np_utils.to_categorical(Y_train, num_classes=2)\n",
    "Y_test = np_utils.to_categorical(Y_test, num_classes=2)\n",
    "\n",
    "print(Y_train[3]) # cat\n",
    "print(Y_train[4002]) # dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some explanation of what I'm doing at each step below:\n",
    "\n",
    "model.add(Conv2d.....) - Adding a convolution layer with x number of filters. The size of each filter is determined by kernel_size. The 'strides' parameter determines by how much I move the filter at each step in the convolution process. If it's (1, 1), it indicates that I slide the filter by 1 pixel on the image at each step. 'padding = same' indicates that, given that my stride is (1, 1), I want the dimensions of my input to be the same as my output. The way it achieves this is by adding a border around the edge of each image (the thickness of the border is determined by the size of the input image and the size of the filter). We set the activation function as 'ReLU' for each convolution layer. \n",
    "\n",
    "model.add(MaxPool2D...) - Adding a pooling layer with the objective of downsizing the input. pool_size(2, 2) indicates that for each 2 x 2 region in the input, we take the maximum in that region, and map it to the corresponding region in the output. \n",
    "\n",
    "model.add(Dropout..) - Applying the Dropout regularization technique with a rate of the provided value\n",
    "\n",
    "model.add(Flatten()) - Converts the input matricies into a 1d vector\n",
    "\n",
    "model.add(Dense(...) - Creates a fully connected layer with the provided value as the number of neurons in the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 50, kernel_size = (3,3), strides = (1,1), padding = 'Same', \n",
    "                 activation ='relu', input_shape = (32,32,3)))\n",
    "model.add(Conv2D(filters = 75, kernel_size = (3,3), strides = (1,1), padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters = 120, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation = \"relu\")) \n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(250, activation = \"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/64 [==============================] - 53s 833ms/step - loss: 0.6868 - accuracy: 0.5531 - val_loss: 0.6609 - val_accuracy: 0.6660\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 55s 862ms/step - loss: 0.6219 - accuracy: 0.6575 - val_loss: 0.5724 - val_accuracy: 0.7105\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 57s 886ms/step - loss: 0.5749 - accuracy: 0.7053 - val_loss: 0.5563 - val_accuracy: 0.7255\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 58s 913ms/step - loss: 0.5256 - accuracy: 0.7354 - val_loss: 0.4982 - val_accuracy: 0.7575\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 54s 851ms/step - loss: 0.4854 - accuracy: 0.7671 - val_loss: 0.5246 - val_accuracy: 0.7375\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 58s 906ms/step - loss: 0.4588 - accuracy: 0.7856 - val_loss: 0.4764 - val_accuracy: 0.7720\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 57s 894ms/step - loss: 0.4272 - accuracy: 0.8058 - val_loss: 0.4632 - val_accuracy: 0.7805\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 50s 774ms/step - loss: 0.4003 - accuracy: 0.8181 - val_loss: 0.4445 - val_accuracy: 0.7905\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 56s 875ms/step - loss: 0.3764 - accuracy: 0.8311 - val_loss: 0.4633 - val_accuracy: 0.7865\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 57s 895ms/step - loss: 0.3442 - accuracy: 0.8455 - val_loss: 0.4615 - val_accuracy: 0.7840\n"
     ]
    }
   ],
   "source": [
    "batch_size = 125\n",
    "history = model.fit(X_train, Y_train, epochs=10, validation_data=(X_test, Y_test), \n",
    "                    steps_per_epoch=X_train.shape[0] // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some tuning, we achieve an accuracy on the testing dataset of 78.4 %."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
